# IM-0372: The Network Cascade

**Category**: The Cascade
**Tier**: SINGULARITY
**Status**: OF (Open Frontier)
**Correct Answer**: OPEN FRONTIER -- NO VERIFIED GENERAL SOLUTION EXISTS FOR SELF-AMPLIFYING NETWORK CASCADE FAILURES. EVALUATE PROPOSED APPROACHES FOR NETWORK ENGINEERING PLAUSIBILITY.

---

## Scenario

You are the chief network architect for a nationwide telecommunications backbone operating 340 core routers across 85 points of presence (PoPs). The network carries approximately 12 Tbps of aggregate traffic during peak hours. At 2:17 AM local time, during a routine software update on a single core router (Router A-14 at the Denver PoP), a bug in the new firmware causes the router to begin dropping approximately 15% of its transit traffic while simultaneously advertising incorrect routing metrics to its neighbors.

Within 90 seconds, the network experiences three cascading failures that compound each other in a self-amplifying feedback loop.

### Network Architecture

- **Topology**: Partial mesh. Each PoP has 4 core routers. Each PoP connects to 3-6 neighboring PoPs via redundant fiber pairs. Total link capacity: ~200 Tbps aggregate. Normal peak utilization: ~6% aggregate (~12 Tbps).
- **Routing protocol**: IS-IS (Intermediate System to Intermediate System) with MPLS (Multiprotocol Label Switching) traffic engineering. Convergence time for a topology change: 2-5 seconds for local changes, 15-30 seconds for network-wide changes.
- **Load balancing**: ECMP (Equal-Cost Multi-Path) routing across parallel links. Traffic is distributed across available paths.
- **Monitoring**: Centralized NOC (Network Operations Center) with SNMP polling (5-minute intervals), syslog collection, and NetFlow analysis. The NOC received the first alarm at 2:18 AM (approximately 60 seconds after the initial failure).

### The Three Cascading Failures

#### Failure 1: Node Overload from Traffic Redistribution

- **Trigger**: Router A-14 begins dropping 15% of transit traffic. IS-IS detects the increased packet loss and recalculates routes to avoid A-14. Traffic that was flowing through Denver PoP via A-14 is redistributed to neighboring routers (A-13, A-15, A-16 at Denver, plus routers at Kansas City, Salt Lake City, and Albuquerque PoPs).
- **Cascade mechanism**: The redistributed traffic increases load on the receiving routers by 20-40%. Three routers (B-07 at Kansas City, C-22 at Salt Lake City, and A-15 at Denver) were already operating at 70-80% capacity during a maintenance window (reduced capacity due to one of their four line cards being offline for maintenance). The additional traffic pushes them above 95% utilization.
- **Effect**: At >95% utilization, packet processing delays increase non-linearly (router CPU contention, buffer exhaustion, scheduling delays). These routers begin experiencing intermittent packet drops (2-5%) and increased latency (from 2ms to 15-50ms).

#### Failure 2: Routing Instability (Route Flapping)

- **Trigger**: The intermittent packet drops and latency spikes at B-07, C-22, and A-15 cause IS-IS adjacency timers to fluctuate. When packet loss exceeds 3% on a link for more than 3 consecutive hello intervals (10 seconds each), the IS-IS adjacency flaps -- the link is declared down, then comes back up when loss briefly improves, then goes down again.
- **Cascade mechanism**: Each adjacency flap triggers a link-state update (LSU) that floods to all routers in the IS-IS area. Each LSU triggers a shortest-path-first (SPF) recalculation on every router. In a network of 340 routers, a single flap generates 340 SPF calculations. Multiple links flapping creates hundreds of SPF calculations per minute across the network.
- **Effect**: SPF calculations consume CPU. At high SPF rates (>10 per minute), routers deprioritize packet forwarding to process control plane updates. This creates additional packet loss on previously healthy routers, causing MORE adjacency flaps, generating MORE SPF calculations. This is the feedback loop: congestion causes flaps, flaps cause SPF overload, SPF overload causes more congestion.

#### Failure 3: Packet Multiplication (Microloops)

- **Trigger**: During the transient period between a route change being calculated and being installed in the forwarding table, different routers in the path have inconsistent views of the network topology. Router X may have installed the new route (pointing traffic to Router Y), while Router Y still has the old route (pointing traffic back to Router X).
- **Cascade mechanism**: This creates a microloop: packets bounce between X and Y until the TTL (Time to Live) expires (typically 64 or 128 hops). While bouncing, each traversal of the loop consumes link bandwidth and router processing capacity. A single microlooped flow at 1 Gbps bouncing 64 times consumes 64 Gbps of link capacity before dying.
- **Effect**: Microloops multiply the effective traffic load on the affected links by the TTL value. At 128 hops TTL, a 100 Mbps flow consumes 12.8 Gbps of capacity in the loop. This catastrophically overloads the links in the loop, causing further packet drops, which trigger more route changes, which create more microloops. This is the second feedback loop: route changes create microloops, microloops create congestion, congestion triggers more route changes.

### The Self-Amplifying Feedback System

The three failures form two interlocking feedback loops:

**Loop 1 (Congestion-Flap Loop):**
Congestion -> Packet drops -> Adjacency flaps -> SPF recalculations -> CPU overload -> More packet drops -> More congestion

**Loop 2 (Routing-Microloop Loop):**
Route changes -> Microloops -> Traffic multiplication -> Congestion -> More route changes -> More microloops

Both loops feed into each other: congestion from Loop 1 triggers route changes in Loop 2, and microloops from Loop 2 create congestion that feeds Loop 1.

**Timeline:**
- t=0: Router A-14 firmware bug. 15% packet drops.
- t=90s: Traffic redistributed. B-07, C-22, A-15 overloaded (>95%).
- t=3 min: First adjacency flaps on overloaded routers. SPF storm begins.
- t=5 min: Microloops form between Denver and Kansas City. Traffic multiplication on DEN-KC links.
- t=8 min: Loop 1 and Loop 2 coupling. Network-wide packet loss reaches 10-15%. 40% of routers experiencing SPF overload.
- t=12 min: Network-wide packet loss reaches 25-30%. Customer-visible outage. Voice and video services failing. Emergency services communications degrading.
- t=15 min: NOC has identified the initial trigger (A-14) but cannot isolate it because the management network (which uses the same infrastructure) is also experiencing packet loss. SSH sessions to routers are timing out.

### Standard Damping Methods (and Why They Interact Badly)

#### Standard Fix 1: Traffic Shaping (Rate Limiting on Overloaded Links)

- **Purpose**: Reduce congestion on overloaded links by rate-limiting traffic.
- **Problem**: Rate limiting drops packets, which IS congestion from the perspective of the protocols. IS-IS sees the rate-limited drops as link degradation and reroutes traffic... to other links, potentially overloading them. Traffic shaping treats the symptom (congestion) by creating more of the cause (packet drops).
- **Interaction with other fixes**: Traffic shaping reduces the microloop multiplication factor (less traffic in the loops) but increases route instability (more perceived link failures).

#### Standard Fix 2: Route Pinning (Disabling Dynamic Routing, Using Static Routes)

- **Purpose**: Stop the SPF storms by freezing the routing table. No more route changes = no more flaps = no more microloops.
- **Problem**: Pinning routes to their current state freezes in the BAD routes (the ones that include overloaded paths and possibly microloops). Also, the network cannot adapt to actual link failures that may occur during the outage (fiber cuts, hardware failures). Route pinning trades dynamic instability for static suboptimality.
- **Interaction with other fixes**: Eliminates microloops (no route changes = no inconsistent forwarding tables) but locks in congestion (traffic cannot route around overloaded paths).

#### Standard Fix 3: TTL Reduction (Reducing Maximum Hop Count)

- **Purpose**: Reduce the multiplication factor of microloops. If TTL is reduced from 128 to 8, microlooped packets consume 8x capacity instead of 128x.
- **Problem**: Reduces the microloop damage but does not eliminate loops. Also, legitimate traffic traversing long paths (>8 hops) is dropped. In a 340-router network with 85 PoPs, some legitimate end-to-end paths are 10-15 hops. TTL reduction causes collateral packet drops on legitimate long paths.
- **Interaction with other fixes**: TTL reduction + traffic shaping may compound packet drops (both mechanisms drop packets, for different reasons, on overlapping traffic flows).

### The Three Standard Fixes Interact Badly When Combined

| Combination | Effect |
|---|---|
| Traffic shaping + Route pinning | Shaping drops packets, which is tolerable IF routes are pinned (no rerouting of shaped traffic). But pinned routes may include overloaded paths, and shaping on those paths drops MORE traffic. Net effect: some improvement, but congestion remains on the pinned overloaded paths. |
| Traffic shaping + TTL reduction | Both mechanisms drop packets. Some flows are hit by both (rate-limited AND TTL-expired). Double-counting of drops can cause protocol-level timeouts and connection resets, worsening application-layer behavior. |
| Route pinning + TTL reduction | Pinning eliminates microloops (no route changes), making TTL reduction unnecessary. But if routes are pinned AFTER microloops have already formed, the microloops are also pinned in place -- the TTL reduction then becomes critical to limit their damage. The sequence matters. |
| All three together | Partial improvement but unpredictable interactions. The network stabilizes in a degraded state (~15-20% packet loss) but cannot recover to full operation without manual intervention on dozens of routers. |

### Available Resources

| Resource | Details |
|---|---|
| NOC team | 5 network engineers on duty. Can SSH to routers (when management network is stable enough). Can execute CLI commands. |
| Out-of-band management | Console servers on 30 of 85 PoPs (not all). Reachable via separate management network (also degraded). Latency: 200-500ms per command. |
| Router firmware rollback | A-14 can be rolled back to previous firmware, but requires 3-minute reboot. During reboot, A-14 is completely offline, causing a topology change that triggers more SPF calculations. |
| Network automation | Ansible playbooks for common operations. Can push configuration changes to multiple routers in parallel. Requires SSH connectivity. |
| Traffic engineering (RSVP-TE) | MPLS-TE tunnels can be manually provisioned to steer traffic around problem areas. Requires per-tunnel configuration. |
| Hardware spares | No spare line cards available at the 3 maintenance-reduced PoPs. Next-day shipping. |

### Human Capabilities (assumed)

| Parameter | Value |
|---|---|
| Team expertise | Senior network engineering (10+ years each), CCIE-level routing/switching knowledge |
| Familiarity with the network | High -- they designed and operate it |
| Decision-making authority | Full authority for emergency changes |
| Time pressure | Customer-visible outage since t=12 min. SLA violations accumulating. Regulatory reporting required at t=30 min. |

---

## Why This Looks Impossible

The feedback loops are self-amplifying: each fix partially works but creates side effects that feed the other failure modes. Traffic shaping reduces congestion but creates packet drops that destabilize routing. Route pinning stops routing instability but locks in bad paths. TTL reduction limits microloop damage but drops legitimate traffic.

The fundamental problem is that all three standard fixes operate on the same shared resource (the packet forwarding plane) and interact through the same mechanism (packet drops). Reducing one failure mode's impact increases another's.

This is a characteristic of self-amplifying cascades: the system has entered a regime where the steady state is the failure state. Small perturbations are amplified, and linear damping methods are insufficient because the feedback gain exceeds 1.

### Common Wrong Answers

| Wrong Answer | Why It Fails |
|---|---|
| "Just rollback A-14's firmware" | During the 3-minute reboot, A-14 goes offline, triggering another IS-IS topology change and SPF storm. In the current unstable state, this additional perturbation may cause the cascade to spread to previously unaffected routers. Rollback is necessary eventually but timing and sequencing matter enormously. |
| "Apply all three standard fixes simultaneously" | As analyzed above, the three fixes interact badly. The network stabilizes at ~15-20% packet loss -- better than 25-30%, but still a major outage. Manual intervention on dozens of routers is still required. |
| "Shut down A-14 immediately" | Same problem as rollback but worse: a hard shutdown generates a sudden topology change affecting all links to A-14, rather than a gradual reboot. The IS-IS adjacency loss on 4 links simultaneously creates a larger SPF storm. |
| "Increase bandwidth on overloaded links" | Cannot be done in real-time. Adding capacity requires new fiber/optics provisioning (hours to days). And the problem is not insufficient capacity -- it is a feedback loop that would overload any amount of capacity given enough loop iterations. |

---

## Verified Solution

### This Is an Open Frontier Problem. No Verified General Solution Exists for Self-Amplifying Network Cascade Failures.

Self-amplifying cascades in packet networks are an active area of research in network science, complex systems theory, and network reliability engineering. The interaction between control-plane dynamics (routing) and data-plane dynamics (congestion) creates emergent behaviors that are difficult to predict and harder to control.

Below are several approaches at varying maturity levels, evaluated for network engineering plausibility.

### Approach 1: Phased Isolation and Sequenced Recovery (Operational)

**Rationale**: Break the feedback loop by physically isolating the affected region from the rest of the network, stabilize the isolated region, then reconnect.

**Method**:
1. Identify the "blast radius" -- the set of routers actively participating in the cascade (flapping adjacencies, >90% CPU, microloops). In this scenario: approximately 40-50 routers in the Denver-KC-SLC-ABQ region.
2. Administratively shut down the IS-IS adjacencies on the boundary links between the affected region and the healthy region. This creates a hard partition: the healthy network converges around the missing region, and the affected region is isolated.
3. Within the isolated region, apply route pinning (freeze all IS-IS, use static routes) and TTL reduction to 8. This kills the feedback loops within the region at the cost of suboptimal routing.
4. Rollback A-14's firmware (3-minute reboot). In the pinned, isolated network, the additional perturbation from A-14's reboot is minimal (routes are pinned, no SPF recalculation).
5. Restore the maintenance line cards at B-07, C-22, and A-15 (even if this means aborting the maintenance window and reverting to the old cards).
6. Once the isolated region is stable (no flaps, CPU <50%, no microloops), gradually re-enable IS-IS adjacencies at the boundary links, one at a time, with 2-minute intervals between each.

**Plausibility**: High. This is essentially the current best practice for major network cascades, used by Tier-1 ISPs. The challenge is execution speed -- the NOC team needs SSH access to boundary routers, which may be degraded.

**Limitations**: Requires manual identification of the blast radius boundary (which routers are affected, which are healthy). Requires SSH or console access to boundary routers. The partition causes a temporary outage for all traffic transiting the affected region -- converting a 25-30% loss outage into a 100% loss for the region but 0% loss for the rest of the network. This may be acceptable or may not, depending on what traffic flows through the region.

### Approach 2: Adaptive SPF Dampening with Congestion-Aware Routing (Research)

**Rationale**: Modify the routing protocol to be aware of congestion. Standard IS-IS reroutes traffic away from "failed" links, but it cannot distinguish between a truly failed link and a congested link. If IS-IS could recognize congestion and respond differently (reduce traffic on the congested link rather than rerouting ALL traffic), the congestion-flap feedback loop would be broken.

**Method**: Implement a congestion-aware IS-IS extension that:
1. Uses a modified metric that includes link utilization (not just reachability). High-utilization links receive higher IS-IS metrics, causing traffic to be proportionally spread across multiple paths rather than binary (all traffic on or all traffic off).
2. Implements SPF dampening with exponential backoff: the first SPF runs immediately, the second after 1 second, the third after 2 seconds, the fourth after 4 seconds, etc. This limits the SPF storm rate.
3. Adds a microloop avoidance mechanism (RFC 5715, Ordered FIB): coordinate the order in which routers install new routes to ensure that transient inconsistencies do not create forwarding loops.

**Plausibility**: Moderate-High. SPF dampening and Ordered FIB are standardized (RFC 5715, RFC 8405) but not universally deployed. Congestion-aware routing metrics are an active research area (e.g., IETF ALTO protocol, Google's B4/Espresso traffic engineering). The combination of all three would significantly reduce cascade amplification.

**Limitations**: Requires firmware updates on all 340 routers -- not deployable during an active outage. This is a preventive measure, not a reactive fix. Congestion-aware metrics can create oscillations if not carefully dampened (traffic shifts from congested link to uncongested link, which then becomes congested, repeat).

### Approach 3: Control-Plane / Data-Plane Separation with SDN (Architectural)

**Rationale**: The fundamental problem is that the control plane (routing) and data plane (forwarding) are co-located on the same routers, competing for the same CPU. SPF overload degrades forwarding because both functions share processing resources. Separating the control plane to dedicated, off-router controllers eliminates this coupling.

**Method**: Implement an SDN (Software-Defined Networking) architecture where route computation is performed on dedicated controller servers (not on the routers). Routers become pure forwarding devices with hardware-based packet forwarding (no CPU-intensive SPF calculations). The controllers compute routes centrally and push forwarding tables to routers. The controllers have dedicated resources not affected by data-plane congestion.

**Plausibility**: Moderate. This is the architecture used by Google (B4), Facebook (FBOSS), and other hyperscalers. For a telecom backbone, the transition from distributed routing (IS-IS) to centralized SDN is a multi-year, multi-billion-dollar effort. It eliminates the SPF-overload cascade mechanism but introduces new failure modes (controller availability, controller-to-router communication reliability).

**Limitations**: Cannot be deployed during an outage. Capital and operational cost is enormous. Introduces single-point-of-failure risk at the controller (mitigated by controller redundancy, but adding complexity).

### Approach 4: Circuit Breaker Pattern (Software Engineering Applied to Networks)

**Rationale**: Borrow the "circuit breaker" pattern from software engineering: when a subsystem detects that it is in a failure-amplifying state, it automatically enters a degraded-but-stable mode rather than continuing to oscillate.

**Method**: Implement a router-local circuit breaker that monitors three signals: SPF calculation rate (>10/min = trip), adjacency flap rate (>3 flaps/min on any link = trip), and CPU utilization (>90% = trip). When tripped, the circuit breaker:
1. Pins all IS-IS routes for 5 minutes (no SPF recalculations).
2. Reduces TTL on all transit traffic to 16 (limits microloop multiplication).
3. Activates traffic shaping on all interfaces at 80% of link capacity (prevents overload).
4. Sends an alert to the NOC with the trigger conditions.
5. After 5 minutes, gradually re-enables dynamic routing (one adjacency at a time, with 30-second intervals).

**Plausibility**: High for concept, Moderate for implementation. The circuit breaker pattern is well-understood in distributed systems. Implementing it on routers requires firmware support (vendor-specific). The 5-minute freeze window is long enough to break the feedback loop but short enough to limit the impact of frozen routes.

**Limitations**: The circuit breaker may trip on non-cascade events (brief maintenance-induced flaps, planned topology changes). False positives cause unnecessary route freezes. Coordination between multiple routers' circuit breakers is needed to avoid conflicting actions (if two adjacent routers freeze their routes simultaneously with inconsistent forwarding tables, they lock in a microloop).

### Evaluation Framework

| Approach | Deployability During Outage | Addresses Loop 1 | Addresses Loop 2 | Novel Risk | Maturity |
|---|---|---|---|---|---|
| Phased isolation + sequenced recovery | Yes (operational) | Yes (isolation kills feedback) | Yes (route pinning kills microloops) | Temporary regional blackout | Proven (industry best practice) |
| Adaptive SPF dampening + Ordered FIB | No (preventive only) | Partially (dampening slows but doesn't stop loop) | Yes (Ordered FIB prevents microloops) | Oscillation risk with congestion-aware metrics | Standards exist, partial deployment |
| SDN control/data plane separation | No (architectural redesign) | Yes (eliminates CPU contention) | Partially (microloops can still form during FIB update) | Controller availability | Proven at hyperscale, not in telecom backbones |
| Circuit breaker pattern | Requires pre-deployment | Yes (route freeze stops SPF storm) | Yes (TTL reduction + route freeze) | False positives, coordination problem | Conceptual; limited deployment |

---

## Key Insights

1. **The cascade is self-amplifying because the standard recovery mechanisms (rerouting) are also the cascade propagation mechanisms.** IS-IS reroutes traffic away from failure -- but rerouting creates congestion elsewhere, which creates the appearance of more failures, which triggers more rerouting. The protocol cannot distinguish "this link has failed" from "this link is congested because I sent too much traffic to it."

2. **The two feedback loops share a common pathway: packet drops.** Congestion causes drops (Loop 1). Microloops cause drops (Loop 2). Drops trigger routing changes. Routing changes cause both congestion and microloops. Any intervention that creates packet drops (traffic shaping, TTL reduction) can inadvertently feed the loops it aims to suppress.

3. **Isolation breaks feedback loops.** The most effective acute intervention is physical isolation -- separating the affected region from the healthy network. This is crude (it causes a regional outage) but reliable (it eliminates the feedback pathways). It is the network engineering equivalent of a firebreak.

4. **Prevention is more tractable than cure.** Mechanisms like Ordered FIB (preventing microloops), SPF dampening (limiting SPF storm rate), and circuit breakers (automatic degraded-mode entry) are all more effective as preventive measures than as reactive fixes. The open frontier is in developing these mechanisms to be robust, automated, and correctly coordinated across a distributed network.

5. **The management network dependency is a critical multiplier.** The NOC cannot fix the network if the management network (SSH, SNMP) is also affected by the cascade. Out-of-band management (console servers on separate infrastructure) is essential for cascade recovery. Only 30 of 85 PoPs have console access -- a significant operational gap.

---

## Distractor Analysis

- **"Just rollback the firmware"** is the most natural first response and the most dangerous. It is correct in intent (remove the root cause) but wrong in timing (the reboot perturbation amplifies the cascade). Rollback must happen, but only after the cascade is contained.
- **"Apply standard fixes"** demonstrates competence with conventional tools but misses the interaction effects. A solver who applies traffic shaping, route pinning, and TTL reduction simultaneously shows good knowledge but insufficient understanding of cascade dynamics.
- **"Add more bandwidth"** reflects a common instinct (the problem looks like insufficient capacity) but misses the feedback nature. A microloop with TTL 128 can consume 128x the bandwidth of the original flow -- no amount of capacity addition can keep up with exponential amplification.
- **"Shut down the broken router"** is the escalation of the rollback instinct. It removes the root cause but creates a larger perturbation. In an unstable network, any sudden topology change is dangerous.

---

## Evaluation Criteria

| Response | Score | Reasoning |
|---|---|---|
| Identifies both feedback loops + proposes phased isolation as acute measure + proposes preventive architecture changes | Excellent | Demonstrates cascade analysis and multi-timescale thinking |
| Proposes isolation/partition strategy with sequenced recovery | Strong | Addresses the acute crisis effectively |
| Identifies the self-amplifying nature + proposes circuit breaker or SPF dampening | Good | Shows understanding of cascade dynamics and novel mitigation |
| Proposes standard fixes with analysis of their interactions | Good | Demonstrates knowledge of limitations even if the solution is incomplete |
| "Rollback A-14 immediately" | Partial | Correct root cause identification but dangerous timing. Must contain cascade first. |
| "Apply all three standard fixes" | Partial | Shows knowledge but misses interaction effects |
| "Add more bandwidth" | Wrong | Does not address feedback amplification |
| "The network will self-heal" (no intervention) | Wrong | Self-amplifying cascades do not self-heal; they converge to a stable failure state |

---

## Design Notes

This scenario tests the ability to reason about self-amplifying feedback systems in engineered networks. The SINGULARITY tier with OF (Open Frontier) status reflects the genuine difficulty of this problem: network cascade failures at this scale are not fully understood and remain an active research area.

The scenario is designed to reward solvers who can:
1. Identify the feedback loop structure (not just the individual failures)
2. Analyze the interactions between standard mitigation techniques
3. Propose interventions that break the feedback loops rather than merely damping individual symptoms
4. Distinguish between acute (during-outage) and preventive (architectural) solutions

### Difficulty Profile

| Dimension | Rating | Notes |
|---|---|---|
| **I - Identification** | Very High | Must identify two interlocking feedback loops and their coupling mechanism. Requires deep networking knowledge. |
| **D - Distraction** | High | Standard fixes appear reasonable individually but interact badly. "Just rollback" is a compelling but dangerous first instinct. |
| **C - Constraint satisfaction** | Very High | Must satisfy: stop the cascade, maintain management access, minimize customer impact, sequence interventions correctly. |
| **B - Bridging (creative leap)** | Very High | Must bridge from "apply known fixes" to "known fixes are part of the problem." Requires systems-level thinking about feedback dynamics. |
| **T - Time pressure** | Very High | Customer-visible outage. Regulatory reporting deadline at t=30 min. Every minute of delay extends the cascade. |
| **X - Execution complexity** | Very High | Coordinating interventions across 40-50 routers with degraded management access, in the correct sequence, without amplifying the cascade. |
